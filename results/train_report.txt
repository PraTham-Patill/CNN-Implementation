=============================
Experiment: LeNet Configurable Model
Dataset: MNIST
Hyperparameters:
  Learning Rate: 0.001
  Batch Size: 64
  Epochs: 10
  Optimizer: ADAM
  Dropout: 0.0
  Weight Decay: 0.0
  Momentum: N/A
  Early Stopping: False
  Patience: N/A
Training Logs:
  Epoch 1: Train Loss=0.2932, Train Acc=91.12%, Val Loss=0.1049, Val Acc=96.78%
  Epoch 2: Train Loss=0.0829, Train Acc=97.39%, Val Loss=0.0903, Val Acc=97.29%
  Epoch 3: Train Loss=0.0574, Train Acc=98.27%, Val Loss=0.0595, Val Acc=98.23%
  Epoch 4: Train Loss=0.0461, Train Acc=98.54%, Val Loss=0.0586, Val Acc=98.25%
  Epoch 5: Train Loss=0.0375, Train Acc=98.76%, Val Loss=0.0435, Val Acc=98.64%
  Epoch 6: Train Loss=0.0317, Train Acc=98.97%, Val Loss=0.0500, Val Acc=98.58%
  Epoch 7: Train Loss=0.0261, Train Acc=99.16%, Val Loss=0.0471, Val Acc=98.75%
  Epoch 8: Train Loss=0.0238, Train Acc=99.24%, Val Loss=0.0417, Val Acc=98.79%
  Epoch 9: Train Loss=0.0206, Train Acc=99.35%, Val Loss=0.0376, Val Acc=98.91%
  Epoch 10: Train Loss=0.0185, Train Acc=99.38%, Val Loss=0.0475, Val Acc=98.61%
Final Test Accuracy: 98.93%
Regularization Used: None
Plot: configurable_mnist_adam_results.png
=============================
=============================
Experiment: LeNet Configurable Model
Dataset: CIFAR10
Hyperparameters:
  Learning Rate: 0.001
  Batch Size: 64
  Epochs: 10
  Optimizer: ADAM
  Dropout: 0.0
  Weight Decay: 0.0
  Momentum: N/A
  Early Stopping: False
  Patience: N/A
Training Logs:
  Epoch 1: Train Loss=1.7668, Train Acc=35.54%, Val Loss=1.5684, Val Acc=43.53%
  Epoch 2: Train Loss=1.4731, Train Acc=47.01%, Val Loss=1.4144, Val Acc=49.24%
  Epoch 3: Train Loss=1.3599, Train Acc=51.45%, Val Loss=1.3405, Val Acc=51.67%
  Epoch 4: Train Loss=1.2814, Train Acc=54.06%, Val Loss=1.3144, Val Acc=52.92%
  Epoch 5: Train Loss=1.2192, Train Acc=56.41%, Val Loss=1.2570, Val Acc=55.10%
  Epoch 6: Train Loss=1.1663, Train Acc=58.84%, Val Loss=1.2191, Val Acc=56.58%
  Epoch 7: Train Loss=1.1152, Train Acc=60.28%, Val Loss=1.2324, Val Acc=56.60%
  Epoch 8: Train Loss=1.0718, Train Acc=61.97%, Val Loss=1.1557, Val Acc=59.32%
  Epoch 9: Train Loss=1.0257, Train Acc=63.73%, Val Loss=1.1517, Val Acc=59.42%
  Epoch 10: Train Loss=0.9881, Train Acc=65.06%, Val Loss=1.1781, Val Acc=59.90%
Final Test Accuracy: 59.28%
Regularization Used: None
Plot: configurable_cifar10_adam_results.png
=============================
=============================
Experiment: LeNet Configurable Model
Dataset: MNIST
Hyperparameters:
  Learning Rate: 0.001
  Batch Size: 64
  Epochs: 15
  Optimizer: ADAM
  Dropout: 0.0
  Weight Decay: 0.01
  Momentum: N/A
  Early Stopping: False
  Patience: N/A
Training Logs:
  Epoch 1: Train Loss=0.3515, Train Acc=89.52%, Val Loss=0.1555, Val Acc=95.50%
  Epoch 2: Train Loss=0.1366, Train Acc=96.04%, Val Loss=0.1538, Val Acc=95.47%
  Epoch 3: Train Loss=0.1213, Train Acc=96.48%, Val Loss=0.1195, Val Acc=96.66%
  Epoch 4: Train Loss=0.1133, Train Acc=96.68%, Val Loss=0.1218, Val Acc=96.54%
  Epoch 5: Train Loss=0.1096, Train Acc=96.80%, Val Loss=0.1140, Val Acc=96.62%
  Epoch 6: Train Loss=0.1072, Train Acc=96.95%, Val Loss=0.1306, Val Acc=96.09%
  Epoch 7: Train Loss=0.1038, Train Acc=97.03%, Val Loss=0.1116, Val Acc=96.64%
  Epoch 8: Train Loss=0.1012, Train Acc=97.08%, Val Loss=0.1088, Val Acc=96.83%
  Epoch 9: Train Loss=0.0994, Train Acc=97.15%, Val Loss=0.1057, Val Acc=96.94%
  Epoch 10: Train Loss=0.1011, Train Acc=97.11%, Val Loss=0.1143, Val Acc=96.82%
  Epoch 11: Train Loss=0.0974, Train Acc=97.26%, Val Loss=0.0970, Val Acc=97.46%
  Epoch 12: Train Loss=0.0975, Train Acc=97.19%, Val Loss=0.1023, Val Acc=97.23%
  Epoch 13: Train Loss=0.0944, Train Acc=97.30%, Val Loss=0.1012, Val Acc=97.20%
  Epoch 14: Train Loss=0.0964, Train Acc=97.23%, Val Loss=0.1058, Val Acc=97.14%
  Epoch 15: Train Loss=0.0957, Train Acc=97.26%, Val Loss=0.1082, Val Acc=96.81%
Final Test Accuracy: 97.26%
Regularization Used: L2 (weight_decay = 0.01)
Plot: configurable_mnist_adam_results.png
=============================
=============================
Experiment: LeNet Configurable Model
Dataset: MNIST
Hyperparameters:
  Learning Rate: 0.001
  Batch Size: 64
  Epochs: 20
  Optimizer: ADAM
  Dropout: 0.0
  Weight Decay: 0.0
  Momentum: N/A
  Early Stopping: False
  Patience: N/A
Training Logs:
  Epoch 1: Train Loss=0.2932, Train Acc=91.12%, Val Loss=0.1049, Val Acc=96.78%
  Epoch 2: Train Loss=0.0829, Train Acc=97.39%, Val Loss=0.0903, Val Acc=97.29%
  Epoch 3: Train Loss=0.0574, Train Acc=98.27%, Val Loss=0.0595, Val Acc=98.23%
  Epoch 4: Train Loss=0.0461, Train Acc=98.54%, Val Loss=0.0586, Val Acc=98.25%
  Epoch 5: Train Loss=0.0375, Train Acc=98.76%, Val Loss=0.0435, Val Acc=98.64%
  Epoch 6: Train Loss=0.0317, Train Acc=98.97%, Val Loss=0.0500, Val Acc=98.58%
  Epoch 7: Train Loss=0.0261, Train Acc=99.16%, Val Loss=0.0471, Val Acc=98.75%
  Epoch 8: Train Loss=0.0238, Train Acc=99.24%, Val Loss=0.0417, Val Acc=98.79%
  Epoch 9: Train Loss=0.0206, Train Acc=99.35%, Val Loss=0.0376, Val Acc=98.91%
  Epoch 10: Train Loss=0.0185, Train Acc=99.38%, Val Loss=0.0475, Val Acc=98.61%
  Epoch 11: Train Loss=0.0160, Train Acc=99.45%, Val Loss=0.0514, Val Acc=98.82%
  Epoch 12: Train Loss=0.0143, Train Acc=99.53%, Val Loss=0.0466, Val Acc=98.89%
  Epoch 13: Train Loss=0.0137, Train Acc=99.54%, Val Loss=0.0446, Val Acc=98.80%
  Epoch 14: Train Loss=0.0118, Train Acc=99.60%, Val Loss=0.0456, Val Acc=98.78%
  Epoch 15: Train Loss=0.0104, Train Acc=99.67%, Val Loss=0.0436, Val Acc=98.86%
  Epoch 16: Train Loss=0.0097, Train Acc=99.69%, Val Loss=0.0558, Val Acc=98.63%
  Epoch 17: Train Loss=0.0103, Train Acc=99.69%, Val Loss=0.0441, Val Acc=98.99%
  Epoch 18: Train Loss=0.0078, Train Acc=99.74%, Val Loss=0.0507, Val Acc=98.84%
  Epoch 19: Train Loss=0.0078, Train Acc=99.75%, Val Loss=0.0494, Val Acc=98.91%
  Epoch 20: Train Loss=0.0066, Train Acc=99.79%, Val Loss=0.0553, Val Acc=98.81%
Final Test Accuracy: 99.00%
Regularization Used: None
Plot: configurable_mnist_adam_results.png
=============================
=============================
Experiment: LeNet Configurable Model
Dataset: CIFAR10
Hyperparameters:
  Learning Rate: 0.001
  Batch Size: 64
  Epochs: 20
  Optimizer: ADAM
  Dropout: 0.0
  Weight Decay: 0.0
  Momentum: N/A
  Early Stopping: False
  Patience: N/A
Training Logs:
  Epoch 1: Train Loss=1.7668, Train Acc=35.54%, Val Loss=1.5684, Val Acc=43.53%
  Epoch 2: Train Loss=1.4731, Train Acc=47.01%, Val Loss=1.4144, Val Acc=49.24%
  Epoch 3: Train Loss=1.3599, Train Acc=51.45%, Val Loss=1.3405, Val Acc=51.67%
  Epoch 4: Train Loss=1.2814, Train Acc=54.06%, Val Loss=1.3144, Val Acc=52.92%
  Epoch 5: Train Loss=1.2192, Train Acc=56.41%, Val Loss=1.2570, Val Acc=55.10%
  Epoch 6: Train Loss=1.1663, Train Acc=58.84%, Val Loss=1.2191, Val Acc=56.58%
  Epoch 7: Train Loss=1.1152, Train Acc=60.28%, Val Loss=1.2324, Val Acc=56.60%
  Epoch 8: Train Loss=1.0718, Train Acc=61.97%, Val Loss=1.1557, Val Acc=59.32%
  Epoch 9: Train Loss=1.0257, Train Acc=63.73%, Val Loss=1.1517, Val Acc=59.42%
  Epoch 10: Train Loss=0.9881, Train Acc=65.06%, Val Loss=1.1781, Val Acc=59.90%
  Epoch 11: Train Loss=0.9539, Train Acc=66.23%, Val Loss=1.1458, Val Acc=60.35%
  Epoch 12: Train Loss=0.9158, Train Acc=67.62%, Val Loss=1.1616, Val Acc=59.81%
  Epoch 13: Train Loss=0.8885, Train Acc=68.55%, Val Loss=1.1762, Val Acc=60.20%
  Epoch 14: Train Loss=0.8570, Train Acc=69.77%, Val Loss=1.1549, Val Acc=60.56%
  Epoch 15: Train Loss=0.8278, Train Acc=70.88%, Val Loss=1.1595, Val Acc=61.06%
  Epoch 16: Train Loss=0.8007, Train Acc=71.43%, Val Loss=1.1939, Val Acc=60.11%
  Epoch 17: Train Loss=0.7763, Train Acc=72.48%, Val Loss=1.2233, Val Acc=59.57%
  Epoch 18: Train Loss=0.7477, Train Acc=73.41%, Val Loss=1.2143, Val Acc=60.86%
  Epoch 19: Train Loss=0.7264, Train Acc=74.19%, Val Loss=1.2629, Val Acc=60.74%
  Epoch 20: Train Loss=0.7060, Train Acc=74.99%, Val Loss=1.2480, Val Acc=60.20%
Final Test Accuracy: 59.74%
Regularization Used: None
Plot: configurable_cifar10_adam_results.png
=============================
=============================
Experiment: LeNet Configurable Model
Dataset: MNIST
Hyperparameters:
  Learning Rate: 0.001
  Batch Size: 64
  Epochs: 15
  Optimizer: ADAM
  Dropout: 0.0
  Weight Decay: 0.01
  Momentum: N/A
  Early Stopping: False
  Patience: N/A
Training Logs:
  Epoch 1: Train Loss=0.3515, Train Acc=89.52%, Val Loss=0.1555, Val Acc=95.50%
  Epoch 2: Train Loss=0.1366, Train Acc=96.04%, Val Loss=0.1538, Val Acc=95.47%
  Epoch 3: Train Loss=0.1213, Train Acc=96.48%, Val Loss=0.1195, Val Acc=96.66%
  Epoch 4: Train Loss=0.1133, Train Acc=96.68%, Val Loss=0.1218, Val Acc=96.54%
  Epoch 5: Train Loss=0.1096, Train Acc=96.80%, Val Loss=0.1140, Val Acc=96.62%
  Epoch 6: Train Loss=0.1072, Train Acc=96.95%, Val Loss=0.1306, Val Acc=96.09%
  Epoch 7: Train Loss=0.1038, Train Acc=97.03%, Val Loss=0.1116, Val Acc=96.64%
  Epoch 8: Train Loss=0.1012, Train Acc=97.08%, Val Loss=0.1088, Val Acc=96.83%
  Epoch 9: Train Loss=0.0994, Train Acc=97.15%, Val Loss=0.1057, Val Acc=96.94%
  Epoch 10: Train Loss=0.1011, Train Acc=97.11%, Val Loss=0.1143, Val Acc=96.82%
  Epoch 11: Train Loss=0.0974, Train Acc=97.26%, Val Loss=0.0970, Val Acc=97.46%
  Epoch 12: Train Loss=0.0975, Train Acc=97.19%, Val Loss=0.1023, Val Acc=97.23%
  Epoch 13: Train Loss=0.0944, Train Acc=97.30%, Val Loss=0.1012, Val Acc=97.20%
  Epoch 14: Train Loss=0.0964, Train Acc=97.23%, Val Loss=0.1058, Val Acc=97.14%
  Epoch 15: Train Loss=0.0957, Train Acc=97.26%, Val Loss=0.1082, Val Acc=96.81%
Final Test Accuracy: 97.26%
Regularization Used: L2 (weight_decay = 0.01)
Plot: configurable_mnist_adam_results.png
=============================
=============================
Experiment: LeNet Configurable Model
Dataset: MNIST
Hyperparameters:
  Learning Rate: 0.001
  Batch Size: 64
  Epochs: 15
  Optimizer: ADAM
  Dropout: 0.5
  Weight Decay: 0.0
  Momentum: N/A
  Early Stopping: False
  Patience: N/A
Training Logs:
  Epoch 1: Train Loss=0.5165, Train Acc=83.80%, Val Loss=0.1181, Val Acc=96.52%
  Epoch 2: Train Loss=0.1734, Train Acc=95.25%, Val Loss=0.0854, Val Acc=97.49%
  Epoch 3: Train Loss=0.1309, Train Acc=96.47%, Val Loss=0.0718, Val Acc=97.95%
  Epoch 4: Train Loss=0.1062, Train Acc=97.14%, Val Loss=0.0615, Val Acc=98.41%
  Epoch 5: Train Loss=0.0901, Train Acc=97.63%, Val Loss=0.0557, Val Acc=98.45%
  Epoch 6: Train Loss=0.0804, Train Acc=97.84%, Val Loss=0.0557, Val Acc=98.55%
  Epoch 7: Train Loss=0.0754, Train Acc=98.01%, Val Loss=0.0468, Val Acc=98.69%
  Epoch 8: Train Loss=0.0652, Train Acc=98.22%, Val Loss=0.0474, Val Acc=98.67%
  Epoch 9: Train Loss=0.0600, Train Acc=98.35%, Val Loss=0.0496, Val Acc=98.79%
  Epoch 10: Train Loss=0.0570, Train Acc=98.47%, Val Loss=0.0426, Val Acc=98.92%
  Epoch 11: Train Loss=0.0525, Train Acc=98.60%, Val Loss=0.0455, Val Acc=98.84%
  Epoch 12: Train Loss=0.0489, Train Acc=98.64%, Val Loss=0.0423, Val Acc=98.93%
  Epoch 13: Train Loss=0.0487, Train Acc=98.68%, Val Loss=0.0529, Val Acc=98.65%
  Epoch 14: Train Loss=0.0462, Train Acc=98.73%, Val Loss=0.0490, Val Acc=98.76%
  Epoch 15: Train Loss=0.0436, Train Acc=98.78%, Val Loss=0.0491, Val Acc=98.89%
Final Test Accuracy: 99.29%
Regularization Used: Dropout (rate = 0.5)
Plot: configurable_mnist_adam_results.png
=============================
=============================
Experiment: LeNet Configurable Model
Dataset: MNIST
Hyperparameters:
  Learning Rate: 0.01
  Batch Size: 64
  Epochs: 15
  Optimizer: SGD
  Dropout: 0.0
  Weight Decay: 0.0
  Momentum: 0.9
  Early Stopping: False
  Patience: N/A
Training Logs:
  Epoch 1: Train Loss=0.5082, Train Acc=83.40%, Val Loss=0.1258, Val Acc=96.12%
  Epoch 2: Train Loss=0.0962, Train Acc=97.02%, Val Loss=0.0808, Val Acc=97.59%
  Epoch 3: Train Loss=0.0651, Train Acc=98.00%, Val Loss=0.0781, Val Acc=97.57%
  Epoch 4: Train Loss=0.0493, Train Acc=98.47%, Val Loss=0.0568, Val Acc=98.34%
  Epoch 5: Train Loss=0.0411, Train Acc=98.69%, Val Loss=0.0510, Val Acc=98.30%
  Epoch 6: Train Loss=0.0332, Train Acc=98.98%, Val Loss=0.0465, Val Acc=98.60%
  Epoch 7: Train Loss=0.0271, Train Acc=99.13%, Val Loss=0.0497, Val Acc=98.59%
  Epoch 8: Train Loss=0.0230, Train Acc=99.29%, Val Loss=0.0448, Val Acc=98.76%
  Epoch 9: Train Loss=0.0210, Train Acc=99.34%, Val Loss=0.0451, Val Acc=98.77%
  Epoch 10: Train Loss=0.0171, Train Acc=99.48%, Val Loss=0.0438, Val Acc=98.79%
  Epoch 11: Train Loss=0.0148, Train Acc=99.54%, Val Loss=0.0446, Val Acc=98.89%
  Epoch 12: Train Loss=0.0134, Train Acc=99.58%, Val Loss=0.0463, Val Acc=98.62%
  Epoch 13: Train Loss=0.0117, Train Acc=99.62%, Val Loss=0.0461, Val Acc=98.67%
  Epoch 14: Train Loss=0.0117, Train Acc=99.60%, Val Loss=0.0509, Val Acc=98.75%
  Epoch 15: Train Loss=0.0109, Train Acc=99.62%, Val Loss=0.0483, Val Acc=98.74%
Final Test Accuracy: 98.81%
Regularization Used: SGD with Momentum (momentum = 0.9)
Plot: configurable_mnist_sgd_results.png
=============================
=============================
Experiment: LeNet Configurable Model
Dataset: MNIST
Hyperparameters:
  Learning Rate: 0.01
  Batch Size: 64
  Epochs: 15
  Optimizer: SGD
  Dropout: 0.5
  Weight Decay: 0.01
  Momentum: 0.9
  Early Stopping: False
  Patience: N/A
Training Logs:
  Epoch 1: Train Loss=0.8353, Train Acc=72.31%, Val Loss=0.1885, Val Acc=94.29%
  Epoch 2: Train Loss=0.2653, Train Acc=92.90%, Val Loss=0.1369, Val Acc=95.92%
  Epoch 3: Train Loss=0.2179, Train Acc=94.17%, Val Loss=0.1350, Val Acc=96.02%
  Epoch 4: Train Loss=0.1998, Train Acc=94.75%, Val Loss=0.1082, Val Acc=96.91%
  Epoch 5: Train Loss=0.1895, Train Acc=94.99%, Val Loss=0.1133, Val Acc=96.82%
  Epoch 6: Train Loss=0.1804, Train Acc=95.25%, Val Loss=0.0975, Val Acc=97.22%
  Epoch 7: Train Loss=0.1794, Train Acc=95.29%, Val Loss=0.1235, Val Acc=96.55%
  Epoch 8: Train Loss=0.1730, Train Acc=95.34%, Val Loss=0.0966, Val Acc=97.28%
  Epoch 9: Train Loss=0.1717, Train Acc=95.50%, Val Loss=0.1041, Val Acc=97.08%
  Epoch 10: Train Loss=0.1715, Train Acc=95.62%, Val Loss=0.1041, Val Acc=97.00%
  Epoch 11: Train Loss=0.1686, Train Acc=95.54%, Val Loss=0.1027, Val Acc=97.13%
  Epoch 12: Train Loss=0.1635, Train Acc=95.77%, Val Loss=0.1075, Val Acc=96.91%
  Epoch 13: Train Loss=0.1666, Train Acc=95.63%, Val Loss=0.1012, Val Acc=97.18%
  Epoch 14: Train Loss=0.1636, Train Acc=95.68%, Val Loss=0.0963, Val Acc=97.13%
  Epoch 15: Train Loss=0.1622, Train Acc=95.76%, Val Loss=0.0992, Val Acc=97.19%
Final Test Accuracy: 97.41%
Regularization Used: Dropout (rate = 0.5), L2 (weight_decay = 0.01), SGD with Momentum (momentum = 0.9)
Plot: configurable_mnist_sgd_results.png
=============================
=============================
Experiment: LeNet Configurable Model
Dataset: CIFAR10
Hyperparameters:
  Learning Rate: 0.001
  Batch Size: 64
  Epochs: 15
  Optimizer: ADAM
  Dropout: 0.0
  Weight Decay: 0.01
  Momentum: N/A
  Early Stopping: False
  Patience: N/A
Training Logs:
  Epoch 1: Train Loss=1.8849, Train Acc=30.68%, Val Loss=1.6720, Val Acc=38.86%
  Epoch 2: Train Loss=1.6155, Train Acc=41.03%, Val Loss=1.5626, Val Acc=43.46%
  Epoch 3: Train Loss=1.5258, Train Acc=44.45%, Val Loss=1.4831, Val Acc=46.75%
  Epoch 4: Train Loss=1.4838, Train Acc=46.04%, Val Loss=1.4885, Val Acc=46.60%
  Epoch 5: Train Loss=1.4547, Train Acc=47.17%, Val Loss=1.4351, Val Acc=48.20%
  Epoch 6: Train Loss=1.4313, Train Acc=48.40%, Val Loss=1.4320, Val Acc=48.47%
  Epoch 7: Train Loss=1.4124, Train Acc=48.58%, Val Loss=1.4037, Val Acc=49.80%
  Epoch 8: Train Loss=1.3955, Train Acc=49.34%, Val Loss=1.3962, Val Acc=49.26%
  Epoch 9: Train Loss=1.3773, Train Acc=50.23%, Val Loss=1.3798, Val Acc=50.41%
  Epoch 10: Train Loss=1.3625, Train Acc=51.02%, Val Loss=1.3581, Val Acc=51.21%
  Epoch 11: Train Loss=1.3490, Train Acc=51.39%, Val Loss=1.3679, Val Acc=51.11%
  Epoch 12: Train Loss=1.3387, Train Acc=51.61%, Val Loss=1.3442, Val Acc=51.82%
  Epoch 13: Train Loss=1.3275, Train Acc=52.30%, Val Loss=1.3902, Val Acc=49.89%
  Epoch 14: Train Loss=1.3187, Train Acc=52.30%, Val Loss=1.3489, Val Acc=51.44%
  Epoch 15: Train Loss=1.3122, Train Acc=52.90%, Val Loss=1.3502, Val Acc=51.67%
Final Test Accuracy: 52.55%
Regularization Used: L2 (weight_decay = 0.01)
Plot: configurable_cifar10_adam_results.png
=============================
=============================
Experiment: LeNet Configurable Model
Dataset: CIFAR10
Hyperparameters:
  Learning Rate: 0.001
  Batch Size: 64
  Epochs: 15
  Optimizer: ADAM
  Dropout: 0.5
  Weight Decay: 0.0
  Momentum: N/A
  Early Stopping: False
  Patience: N/A
Training Logs:
  Epoch 1: Train Loss=1.9353, Train Acc=28.09%, Val Loss=1.6763, Val Acc=38.84%
  Epoch 2: Train Loss=1.6625, Train Acc=39.05%, Val Loss=1.4996, Val Acc=45.20%
  Epoch 3: Train Loss=1.5488, Train Acc=43.78%, Val Loss=1.4320, Val Acc=47.89%
  Epoch 4: Train Loss=1.4893, Train Acc=46.32%, Val Loss=1.3627, Val Acc=50.89%
  Epoch 5: Train Loss=1.4366, Train Acc=48.03%, Val Loss=1.3326, Val Acc=52.62%
  Epoch 6: Train Loss=1.3994, Train Acc=49.99%, Val Loss=1.2823, Val Acc=54.07%
  Epoch 7: Train Loss=1.3669, Train Acc=51.22%, Val Loss=1.2560, Val Acc=54.76%
  Epoch 8: Train Loss=1.3393, Train Acc=52.32%, Val Loss=1.2541, Val Acc=54.94%
  Epoch 9: Train Loss=1.3146, Train Acc=53.23%, Val Loss=1.2399, Val Acc=55.64%
  Epoch 10: Train Loss=1.2977, Train Acc=54.23%, Val Loss=1.2217, Val Acc=56.38%
  Epoch 11: Train Loss=1.2755, Train Acc=54.95%, Val Loss=1.2067, Val Acc=56.44%
  Epoch 12: Train Loss=1.2613, Train Acc=55.43%, Val Loss=1.1890, Val Acc=57.47%
  Epoch 13: Train Loss=1.2404, Train Acc=56.17%, Val Loss=1.1634, Val Acc=58.38%
  Epoch 14: Train Loss=1.2332, Train Acc=56.52%, Val Loss=1.1568, Val Acc=59.03%
  Epoch 15: Train Loss=1.2232, Train Acc=56.80%, Val Loss=1.1675, Val Acc=58.14%
Final Test Accuracy: 58.33%
Regularization Used: Dropout (rate = 0.5)
Plot: configurable_cifar10_adam_results.png
=============================
=============================
Experiment: Improved LeNet Model
Dataset: MNIST
Hyperparameters:
  Learning Rate: 0.01
  Batch Size: 128
  Epochs: 10
  Optimizer: SGD
  Dropout: 0.5
  Weight Decay: 0.0005
  Momentum: 0.9
  Early Stopping: False
  Patience: N/A
  LR Scheduler: StepLR (step=15, gamma=0.1)
Training Logs:
  Epoch 1: Train Loss=0.6573, Train Acc=78.45%, Val Loss=0.1108, Val Acc=96.54%
  Epoch 2: Train Loss=0.1318, Train Acc=96.27%, Val Loss=0.0909, Val Acc=97.36%
  Epoch 3: Train Loss=0.0932, Train Acc=97.44%, Val Loss=0.0552, Val Acc=98.24%
  Epoch 4: Train Loss=0.0733, Train Acc=98.01%, Val Loss=0.0501, Val Acc=98.53%
  Epoch 5: Train Loss=0.0598, Train Acc=98.30%, Val Loss=0.0493, Val Acc=98.68%
  Epoch 6: Train Loss=0.0514, Train Acc=98.58%, Val Loss=0.0413, Val Acc=98.83%
  Epoch 7: Train Loss=0.0452, Train Acc=98.71%, Val Loss=0.0391, Val Acc=98.87%
  Epoch 8: Train Loss=0.0413, Train Acc=98.90%, Val Loss=0.0375, Val Acc=98.93%
  Epoch 9: Train Loss=0.0355, Train Acc=99.00%, Val Loss=0.0359, Val Acc=99.06%
  Epoch 10: Train Loss=0.0329, Train Acc=99.12%, Val Loss=0.0425, Val Acc=98.87%
Best Validation Accuracy: 99.06% (Epoch 9)
Final Test Loss: 0.0264
Final Test Accuracy: 99.17%
Total Training Time: 11.84 minutes
Regularization Used: Dropout (rate = 0.5), L2 (weight_decay = 0.0005), SGD with Momentum (momentum = 0.9)
Plot: ImprovedLeNet_mnist_sgd_dropout0.5_wd5e-04_10epochs.png
=============================
=============================
Experiment: Improved LeNet Model
Dataset: CIFAR10
Hyperparameters:
  Learning Rate: 0.01
  Batch Size: 128
  Epochs: 15
  Optimizer: SGD
  Dropout: 0.5
  Weight Decay: 0.0005
  Momentum: 0.9
  Early Stopping: False
  Patience: N/A
  LR Scheduler: StepLR (step=15, gamma=0.1)
Training Logs:
  Epoch 1: Train Loss=2.0518, Train Acc=22.44%, Val Loss=1.8749, Val Acc=29.11%
  Epoch 2: Train Loss=1.7267, Train Acc=35.33%, Val Loss=1.5487, Val Acc=42.51%
  Epoch 3: Train Loss=1.5548, Train Acc=42.58%, Val Loss=1.4059, Val Acc=48.17%
  Epoch 4: Train Loss=1.4207, Train Acc=48.49%, Val Loss=1.2682, Val Acc=53.81%
  Epoch 5: Train Loss=1.3035, Train Acc=53.10%, Val Loss=1.1381, Val Acc=58.44%
  Epoch 6: Train Loss=1.2004, Train Acc=57.53%, Val Loss=1.0811, Val Acc=61.46%
  Epoch 7: Train Loss=1.1273, Train Acc=60.75%, Val Loss=1.0523, Val Acc=62.87%
  Epoch 8: Train Loss=1.0696, Train Acc=62.62%, Val Loss=0.9562, Val Acc=66.70%
  Epoch 9: Train Loss=1.0110, Train Acc=64.76%, Val Loss=0.8894, Val Acc=68.63%
  Epoch 10: Train Loss=0.9676, Train Acc=66.57%, Val Loss=0.8827, Val Acc=69.60%
  Epoch 11: Train Loss=0.9277, Train Acc=68.34%, Val Loss=0.9255, Val Acc=68.05%
  Epoch 12: Train Loss=0.8960, Train Acc=69.56%, Val Loss=0.8182, Val Acc=71.50%
  Epoch 13: Train Loss=0.8592, Train Acc=70.87%, Val Loss=0.8221, Val Acc=71.74%
  Epoch 14: Train Loss=0.8343, Train Acc=71.86%, Val Loss=0.7927, Val Acc=72.05%
  Epoch 15: Train Loss=0.8189, Train Acc=72.37%, Val Loss=0.7671, Val Acc=73.58%
Best Validation Accuracy: 73.58% (Epoch 15)
Final Test Loss: 0.7305
Final Test Accuracy: 74.93%
Total Training Time: 16.33 minutes
Regularization Used: Dropout (rate = 0.5), L2 (weight_decay = 0.0005), SGD with Momentum (momentum = 0.9)
Plot: ImprovedLeNet_cifar10_sgd_dropout0.5_wd5e-04_15epochs.png
=============================


================================================================================
COMPREHENSIVE MODEL COMPARISON TABLE
Generated on: 2025-09-04 19:26:50
================================================================================

MNIST DATASET RESULTS:
--------------------------------------------------------------------------------
Model                               Optimizer  Dropout  Weight Decay Test Acc   Best Val Acc Time (min)
--------------------------------------------------------------------------------
LeNet Configurable Model            ADAM       0.5      0.0          99.29%     0.00%        0.0       
Improved LeNet Model                SGD        0.5      0.0005       99.17%     99.06%       11.8      
LeNet Configurable Model            ADAM       0.0      0.0          99.00%     0.00%        0.0       
LeNet Configurable Model            ADAM       0.0      0.0          98.93%     0.00%        0.0       
LeNet Configurable Model            SGD        0.0      0.0          98.81%     0.00%        0.0       
LeNet Configurable Model            SGD        0.5      0.01         97.41%     0.00%        0.0       
LeNet Configurable Model            ADAM       0.0      0.01         97.26%     0.00%        0.0       
LeNet Configurable Model            ADAM       0.0      0.01         97.26%     0.00%        0.0       

--------------------------------------------------------------------------------
MNIST Best Performance: 99.29%

CIFAR-10 DATASET RESULTS:
--------------------------------------------------------------------------------
Model                               Optimizer  Dropout  Weight Decay Test Acc   Best Val Acc Time (min)
--------------------------------------------------------------------------------
Improved LeNet Model                SGD        0.5      0.0005       74.93%     73.58%       16.3      
LeNet Configurable Model            ADAM       0.0      0.0          59.74%     0.00%        0.0       
LeNet Configurable Model            ADAM       0.0      0.0          59.28%     0.00%        0.0       
LeNet Configurable Model            ADAM       0.5      0.0          58.33%     0.00%        0.0       
LeNet Configurable Model            ADAM       0.0      0.01         52.55%     0.00%        0.0       

--------------------------------------------------------------------------------
CIFAR-10 Best Performance: 74.93%

SUMMARY STATISTICS:
--------------------------------------------------------------------------------
Total Experiments: 13
MNIST Experiments: 8
CIFAR-10 Experiments: 5

MNIST Average Accuracy: 98.39%
CIFAR-10 Average Accuracy: 60.97%

Best Overall MNIST Model: LeNet Configurable Model
Best Overall CIFAR-10 Model: Improved LeNet Model

KEY INSIGHTS:
- Improved LeNet architecture achieves ~75% on CIFAR-10 (significant improvement over basic LeNet ~60%)
- MNIST shows excellent performance across all methods (97-99% accuracy)
- Dropout regularization shows strong performance on both datasets
- SGD with momentum provides competitive results with proper learning rate scheduling
- Data augmentation (CIFAR-10) and enhanced architecture contribute significantly to performance

================================================================================
